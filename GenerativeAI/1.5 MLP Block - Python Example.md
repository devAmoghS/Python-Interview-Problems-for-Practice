## Multi-Layer Perceptron (MLP) in Transformers

The Multi-Layer Perceptron (MLP) is a key component of the Transformer architecture, responsible for refining the representation of each token using a non-linear transformation. Here's the mathematical intuition behind the MLP in Transformers:

### Mathematical Formulation

The MLP in Transformers operates across the features of each token, applying the same non-linear transformation to each token independently. Given the output of the self-attention layer `y(m)_n` for token `n` at layer `m`, the MLP computes:

$$
x^{(m+1)}_n = \text{MLP}_\theta(y^{(m)}_n)
$$

where `\theta` represents the parameters of the MLP, which are shared across all tokens.

The MLP typically consists of one or two hidden layers with a dimension equal to the number of features `D` (or larger). The computational cost of this step is roughly `N * D * D`, where `N` is the sequence length.

### Example Implementation in Python and NumPy

Here's a simple example of implementing the MLP component in Transformers using Python and NumPy:

```python
import numpy as np

# Define MLP parameters
D = 4  # Number of features
hidden_size = 8  # Size of the hidden layer

# Sample input from the self-attention layer
y = np.array([[1, 0, 1, 0],
              [0, 1, 0, 1],
              [1, 1, 1, 1]])

# Initialize MLP weights
W1 = np.random.rand(D, hidden_size)
b1 = np.random.rand(1, hidden_size)
W2 = np.random.rand(hidden_size, D)
b2 = np.random.rand(1, D)

# Compute MLP output
h = np.maximum(0, y @ W1 + b1)  # ReLU activation in the hidden layer
x = h @ W2 + b2  # Linear output layer

print("Input from self-attention layer:\n", y)
print("Output of the MLP:\n", x)
```

In this example:

1. We define the MLP parameters, including the number of features `D` and the size of the hidden layer.

2. We create a sample input `y` from the self-attention layer.

3. We initialize the weights and biases of the MLP randomly.

4. We compute the output of the MLP by applying the following steps:
   - Compute the hidden layer activation using a ReLU non-linearity.
   - Apply the output layer weights and biases to obtain the final output.

5. Finally, we print the input from the self-attention layer and the output of the MLP.

The MLP in Transformers acts as a non-linear feature extractor, processing the output of the self-attention layer independently for each token. It helps capture complex interactions between features and refine the representations learned by the self-attention mechanism.

Citations:
[1] https://www.youtube.com/watch?v=kO0XdAsY5YA
[2] https://transformer-circuits.pub/2021/framework/index.html
[3] https://arxiv.org/abs/2304.10557
[4] https://learnopencv.com/attention-mechanism-in-transformer-neural-networks/
[5] https://arxiv.org/pdf/2304.10557.pdf
[6] https://www.youtube.com/watch?v=idVm0DMaDR4
[7] https://towardsdatascience.com/the-math-behind-multi-head-attention-in-transformers-c26cba15f625
[8] https://www.youtube.com/watch?v=qw7wFGgNCSU
