To illustrate the functioning of the attention block in Transformers, let's break down the process using a sample sentence, its embedding vector, and the attention mechanism step-by-step. We will also show how to compute the probability distribution of the next best word based on the attention scores.

### Example Sentence

Let's take the sentence: **"Life is short"**.

### Step 1: Word Embedding

First, we need to convert the words into embedding vectors. For simplicity, we'll use random embeddings for each word.

```python
import numpy as np

# Define the sentence and create a dictionary for word indices
sentence = "Life is short"
words = sentence.split()
word_to_index = {word: i for i, word in enumerate(words)}

# Create random embeddings for each word
embedding_dim = 4  # Dimension of the embedding
embeddings = np.random.rand(len(words), embedding_dim)

print("Word Indices:", word_to_index)
print("Word Embeddings:\n", embeddings)
```

### Step 2: Compute Queries, Keys, and Values

In the attention mechanism, we need to compute the queries (Q), keys (K), and values (V) from the embeddings. We will use learned weight matrices for this purpose.

```python
# Initialize weight matrices for Q, K, and V
W_Q = np.random.rand(embedding_dim, embedding_dim)
W_K = np.random.rand(embedding_dim, embedding_dim)
W_V = np.random.rand(embedding_dim, embedding_dim)

# Compute Q, K, V
Q = embeddings @ W_Q
K = embeddings @ W_K
V = embeddings @ W_V

print("Queries (Q):\n", Q)
print("Keys (K):\n", K)
print("Values (V):\n", V)
```

### Step 3: Compute Attention Scores

Next, we calculate the attention scores using the dot product of the queries and keys, followed by a softmax to obtain the attention weights.

```python
# Compute attention scores
scores = Q @ K.T / np.sqrt(embedding_dim)  # Scale by the square root of the dimension
attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)  # Softmax

print("Attention Scores:\n", scores)
print("Attention Weights:\n", attention_weights)
```

### Step 4: Compute Output of the Attention Block

The output of the attention block is computed as a weighted sum of the values, using the attention weights.

```python
# Compute the output of the attention block
output = attention_weights @ V

print("Output of Attention Block:\n", output)
```

### Step 5: Probability Distribution for Next Word

To predict the next word, we can apply a simple linear layer followed by a softmax function to the output of the attention block. This simulates how we would generate probabilities for the next word in a sequence.

```python
# Initialize weights for the output layer
W_out = np.random.rand(embedding_dim, len(words))

# Compute logits
logits = output @ W_out

# Compute probabilities using softmax
probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)

print("Logits:\n", logits)
print("Probability Distribution for Next Word:\n", probabilities)
```

### Summary of the Process

1. **Word Embedding**: Convert words into embedding vectors.
2. **Compute Q, K, V**: Use learned weight matrices to compute queries, keys, and values from the embeddings.
3. **Attention Scores**: Calculate scores using the dot product of queries and keys, then apply softmax to obtain attention weights.
4. **Output of Attention Block**: Compute the output as a weighted sum of the values based on the attention weights.
5. **Next Word Probability**: Generate a probability distribution for the next word using a linear transformation followed by softmax.

### Final Output

The final output will show the probability distribution of the next best word based on the attention mechanism applied to the input sentence. This allows the model to capture the context and relationships between the words effectively.

Citations:
[1] https://nlp.gluon.ai/examples/sentence_embedding/self_attentive_sentence_embedding.html
[2] https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html
[3] https://datascience.stackexchange.com/questions/95134/how-to-encode-a-sentence-using-an-attention-mechanism
[4] https://towardsdatascience.com/contextual-transformer-embeddings-using-self-attention-explained-with-diagrams-and-python-code-d7a9f0f4d94e?gi=0dee21177e82
[5] https://github.com/gazelle93/Transformer-Various-Positional-Encoding
[6] https://www.linkedin.com/pulse/deep-dive-positional-encodings-transformer-neural-network-ajay-taneja
[7] https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021?gi=4b6a109307fe
[8] https://stackoverflow.blog/2023/11/09/an-intuitive-introduction-to-text-embeddings/
